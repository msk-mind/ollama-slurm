# Qwen3 Coder 30B - V100 Profile
# V100: 16GB VRAM, max 4 per node
# Model: ~32GB (Q8_0) split across 4 GPUs (8GB/GPU)
# KV cache at 32K: ~6.5GB total (1.6GB/GPU)
# Total per GPU: ~9.6GB -> fits on V100 16GB
# P40: model too large (32GB exceeds 24GB single GPU)
MODEL_FILE="~/.cache/llama.cpp/qwen3-coder-30b-a3b-instruct-q8_0.gguf"
CPUS=16
MEM="64G"
GPUS=4
GPU_TYPE="v100"
CONTEXT_SIZE=32768
N_GPU_LAYERS=-1
EXTRA_ARGS=""
