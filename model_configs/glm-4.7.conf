# GLM-4.7 Flash Model Configuration
# Requires: 2x A100 80GB (model ~16GB + 13GB KV cache per GPU)
# Will NOT work on V100 16GB or P40 (max 1/node)
MODEL_FILE="~/.cache/llama.cpp/GLM-4.7-Flash-UD-Q4_K_XL.gguf"
CPUS=8
MEM="32G"
GPUS=2
GPU_TYPE="a100"  # a100 (80GB), v100 (16GB), or p40 (23GB, max 1/node)
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS="--reasoning-budget 0 --temp 1.0 --top-p 0.95 --min-p 0.01 --override-kv deepseek2.expert_gating_func=int:2"
