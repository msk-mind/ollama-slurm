# GLM-4.7 Flash Model Configuration
# Requires: 2x A100 80GB (model ~16GB + 13GB KV cache per GPU)
# Will NOT work on V100 16GB or single A100
MODEL_FILE="~/.cache/llama.cpp/GLM-4.7-Flash-UD-Q4_K_XL.gguf"
CPUS=8
MEM="32G"
GPUS=2
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS="--reasoning-budget 0 --temp 1.0 --top-p 0.95 --min-p 0.01 --override-kv deepseek2.expert_gating_func=int:2"
