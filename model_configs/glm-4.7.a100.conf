# GLM-4.7 Flash - A100 Profile
# A100: 80GB VRAM, max 4 per node
# DeepSeek2 MLA architecture: 47 layers, 64 experts, kv_lora_rank=512
# Model: ~17GB, but MoE compute buffers + MLA overhead require >80GB at 128K
# Needs 2x A100
MODEL_FILE="~/.cache/llama.cpp/GLM-4.7-Flash-UD-Q4_K_XL.gguf"
CPUS=12
MEM="48G"
GPUS=2
GPU_TYPE="a100"
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS="--reasoning-budget 0 --temp 1.0 --top-p 0.95 --min-p 0.01 --override-kv deepseek2.expert_gating_func=int:2"
