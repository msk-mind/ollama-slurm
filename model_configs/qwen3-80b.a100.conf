# Qwen3 Next 80B - A100 Profile
# A100: 80GB VRAM, max 4 per node
# Model: ~47GB split across 4 GPUs (11.75GB/GPU)
# KV cache at 128K: ~52GB total (13GB/GPU)
# Total per GPU: ~24.75GB -> fits on A100 80GB
# V100/P40: model too large (47GB exceeds 4x16GB usable capacity)
MODEL_FILE="~/.cache/llama.cpp/Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf"
CPUS=16
MEM="96G"
GPUS=4
GPU_TYPE="a100"
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
