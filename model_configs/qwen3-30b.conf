# Qwen3 30B Model Configuration
# Requires: 2x A100 80GB (preferred) or 4x V100 16GB
# Model: ~18GB + KV cache: ~13GB per GPU at 128K context
MODEL_FILE="~/.cache/llama.cpp/unsloth_Qwen3-30B-A3B-Instruct-2507-GGUF_Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf"
CPUS=16
MEM="64G"
GPUS=2
GPU_TYPE="a100"  # a100 (80GB), v100 (16GB), or p40 (23GB, max 1/node)
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
