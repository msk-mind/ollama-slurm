# Qwen3 Next 80B Model Configuration
# Requires: 4x A100 80GB (will NOT fit on V100s)
# Model: ~47GB + KV cache: ~13GB per GPU at 128K context
MODEL_FILE="~/.cache/llama.cpp/Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf"
CPUS=24
MEM="128G"
GPUS=4
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
