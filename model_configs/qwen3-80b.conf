# Qwen3 Next 80B Model Configuration
# Requires: 4x A100 80GB (will NOT fit on V100 or P40)
# Model: ~47GB + KV cache: ~13GB per GPU at 128K context
MODEL_FILE="~/.cache/llama.cpp/Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf"
CPUS=24
MEM="128G"
GPUS=4
GPU_TYPE="a100"  # a100 (80GB), v100 (16GB), or p40 (23GB, max 1/node)
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
