# Qwen3 30B - V100 Profile
# V100: 16GB VRAM, max 4 per node
# Model: ~18GB split across 4 GPUs (4.5GB/GPU)
# KV cache at 64K: ~13GB total (3.25GB/GPU)
# Total per GPU: ~7.75GB -> fits on V100 16GB
MODEL_FILE="~/.cache/llama.cpp/unsloth_Qwen3-30B-A3B-Instruct-2507-GGUF_Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf"
CPUS=16
MEM="48G"
GPUS=4
GPU_TYPE="v100"
CONTEXT_SIZE=65536
N_GPU_LAYERS=-1
EXTRA_ARGS=""
