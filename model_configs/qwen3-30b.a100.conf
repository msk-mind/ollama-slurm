# Qwen3 30B - A100 Profile
# A100: 80GB VRAM, max 4 per node
# Model: ~18GB, KV cache: ~26GB total at 128K context
# Total VRAM: ~44GB -> fits on 1x A100 with headroom
MODEL_FILE="~/.cache/llama.cpp/unsloth_Qwen3-30B-A3B-Instruct-2507-GGUF_Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf"
CPUS=8
MEM="32G"
GPUS=1
GPU_TYPE="a100"
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
