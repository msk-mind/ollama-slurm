# Qwen3 Coder 30B Model Configuration
# Requires: 2x A100 80GB (Q8_0 is larger than Q4_K_M, won't fit on V100 or P40)
# Model: ~32GB + KV cache: ~13GB per GPU at 128K context
MODEL_FILE="~/.cache/llama.cpp/qwen3-coder-30b-a3b-instruct-q8_0.gguf"
CPUS=16
MEM="64G"
GPUS=2
GPU_TYPE="a100"  # a100 (80GB), v100 (16GB), or p40 (23GB, max 1/node)
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
