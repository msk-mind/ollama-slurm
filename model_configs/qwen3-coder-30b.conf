# Qwen3 Coder 30B Model Configuration
# Requires: 2x A100 80GB or 4x V100 32GB (Q8_0 is larger than Q4_K_M)
# Model: ~32GB + KV cache: ~13GB per GPU at 128K context
MODEL_FILE="~/.cache/llama.cpp/qwen3-coder-30b-a3b-instruct-q8_0.gguf"
CPUS=16
MEM="64G"
GPUS=2
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
