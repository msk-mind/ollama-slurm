# Qwen3 Coder 30B - A100 Profile
# A100: 80GB VRAM, max 4 per node
# Model: ~32GB (Q8_0 quantization), KV cache: ~26GB total at 128K context
# Total VRAM: ~58GB -> fits on 1x A100 with headroom
MODEL_FILE="~/.cache/llama.cpp/qwen3-coder-30b-a3b-instruct-q8_0.gguf"
CPUS=8
MEM="48G"
GPUS=1
GPU_TYPE="a100"
CONTEXT_SIZE=131072
N_GPU_LAYERS=-1
EXTRA_ARGS=""
